# Offline LLM using Ollama

This project focuses on running local LLMs using ollama. Getting user input, providing to the LLM and returning the response from the LLM.

- it has been tested using Qwen2-0.5B and Llama-3
- to run the project ensure you have an LLM running using ollama, update the LLM name in the `createInputQuery` in the /utils directory
